### 目录
- requests 
    - requests安装
    - 基本使用
    - request的7个主要方法
    - Requests常见的异常
    - requests.requests
    - requests.get
    - ...
    - robots协议
- BeautifulSoup 
    - 解析HTML页面
- HTTP协议详解
- Re正则 
    - 正则表达式详解 提取页面关键信息
- Scrapy 
    - 网络爬虫原理介绍
    - 专业爬虫框架介绍
- 项目

### requests库
* 官网：request库：http://www.python-requests.org/en/master/
* request的使用教程：http://www.cnblogs.com/lilinwei340/p/6417689.html

* requests安装  
    - 管理员权限打开cmd，
    - 执行：pip install requests

* 基本使用
    ```PYTHON
        import requests
        try:
            r = requests.get("http://www.baidu.com") #返回response对象
            r.raise_for_status()
            r.encoding = r.apparent_encoding    # 修改编码
            print(r.status_code) #状态码
            print(r.headers['content-type']) # 返回头部信息
            print(r.encoding) # 编码信息
            print(r.text)   #内容部分(存在编码问题)
            print(r.content) #二进制显示
        except:
            print("请求失败")
    ```

* request的7个主要方法
    |A|B|
    |:--:|:--:|
    requests.request() |构造一个请求，支撑以下各方法的基础方法
    requests.get() |获取HTML网页的主要方法，对应于HTTP的GET
    requests.head() |获取HTML网页头信息的方法，对应于HTTP的HEAD
    requests.post() |向HTML网页提交POST请求的方法，对应于HTTP的POST
    requests.put() |向HTML网页提交PUT请求的方法，对应于HTTP的PUT
    requests.patch() |向HTML网页提交局部修改请求，对应于HTTP的PATCH
    requests.delete() |向HTML页面提交删除请求，对应于HTTP的DELETE

* Requests常见的异常
    * r.raise_for_status() 如果不是200，产生异常requests.HTTPError  
    在方法内部判断r.status_code是否等于200，不需要增加额外的if语句，该语句便于利用try‐except进行异常处理
    * 
    |A|B|
    |:--:|:--:|
    requests.ConnectionError |网络连接错误异常，如DNS查询失败、拒绝连接等
    requests.HTTPError |HTTP错误异常
    requests.URLRequired |URL缺失异常
    requests.TooManyRedirects |超过最大重定向次数，产生重定向异常
    requests.ConnectTimeout |连接远程服务器超时异常
    requests.Timeout |请求URL超时，产生超时异常

* requests.requests(method, url, **kwargs) 最基础的方法，返回response对象
    * method 
        - "GET"
        - "HEAD"
        - "POST"
        - "PUT"
        - "PATCH"
        - "DELETE"
        - "OPTIONS"

    * **kwargs 控制访问的参数，均为可选项
        |A|B|
        |:--:|:--:
        params      |字典或字节序列，作为参数传入url
        data	    |字典，字节序列或文件对象作为requests内容
        json	    |json格式的数据，作为request的内容
        headers     |字典，HTTP定制头 User-Agent chrome/50
        timeout	    |超时时间，秒为单位
        cookies     |字典或CookieJar，Request中的cookie
        auth        |元组，支持HTTP认证
        files       |字典类型，传输文件
        proxies     |字典类型，设置代理服务器，可以增加登录认证
        allow_redirects |True/False,是否允许跳转,重定向开关，默认为True
        stream      |True/False，默认为True,获取内容立即下载 
        verify      |True/False, 默认为True,认证SSL证书的开关
        cert        |本地SSL证书路径

* requests.get(url, params=None, **kwargs)  返回response对象  
    - url : 拟获取页面的url链接  
    - params : url中的额外参数，字典或字节流格式，可选  
    - **kwargs: 12个控制访问的参数
    - return: requests.Respond
    0. 状态码 r.status_code
        ```PYTHON
            url = "http://www.baidu.com"
            r = requests.get(url)
            print(r.status_code)  # HTTP请求的返回状态，200表示连接成功，404表示失败
            # 200
        ```
    1. 获取相应的内容：  r.text, r.content
        ```PYTHON
        url = "http://www.baidu.com"
        r = requests.get(url)
        print(r.text)  # HTTP响应内容的字符串形式,   
        print(r.content) # HTTP响应内容的二进制形式  
        ```
    2. 获取响应头内容: r.headers 以字典形式返回
        ```PYTHON
            #以字典形式返回
            print(r.headers)
            #{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Thu, 13 Sep 2018 11:53:41 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}

            >>> r.headers['content-type'] # 字典方法
            >>> r.headers.get('content-type') # 字典方法
        ```

    3. 获取网页编码:  r.encoding, r.apparent_encoding
        ```PYTHON
        url = "http://www.baidu.com"
        r = requests.get(url)

        #HTTP header中猜测的响应内容编码方式,  
        # 如果header中不存在charset，则认为编码为ISO‐8859‐1
        # r.text根据r.encoding显示网页内容 
        print(r.encoding)
        # 'ISO-8859-1'

        # 内容中分析出的响应内容编码方式, 可当做备选编码方式
        print(r.apparent_encoding)  
        # 'utf-8'
        ``` 


    4. json数据  r.json()
        ```PYTHON
        import requests
        r = requests.get("http://ip.taobao.com/service/getIpInfo.php?ip=110.84.226.101")#查看ip
        print(r.json())
        print(r.json()['data']['city'])
        ```

        * 网页状态码: r.status_code 

        * URLs中传递参数：params
            >>> payload = {'key1': 'value1', 'key2': 'value2', 'key3': None}
            >>> r = requests.get('http://httpbin.org/get', params=payload)

            http://httpbin.org/get?key2=value2&key1=value1

        * 设置超时时间: timeout
            >>> r = requests.get(url, timeout=3)

        * 代理访问: proxies
            >>> proxies = {"http":"http://10.10.1.10:3128"，
                    "http":"http://user:pass@10.10.1.10:3128"
                }
            >>> r = requests.get(url, proxies = proxies)

        * 自定义请求头文件 headers
            >>>  r = requests.get(url)
            >>>  r.request.headers
            >>>  hd = {'User-Agent':'chrome/52'}
            >>>  r = requests.get(url, headers = hd)
            >>>  r.request.headers

        * 跳转 allow_redirects 默认是True
            >>>  r = requests.get('http://httpbin.org/cookies/set?k2=v2&k1=v1')
            >>> r.url
            >>> 'http://httpbin.org/cookies'
            >>> r.status_code
            >>> 200
            >>> r.history
            >>> [<Response [302]>]

            >>>  r = requests.get('http://httpbin.org/cookies/set?k2=v2&k1=v1', allow_redirects=False) #禁止跳转
            >>> r.status_code
            >>> 302
            >> r.url
            >>> 'http://httpbin.org/cookies/set?k2=v2&k1=v1'

        * session
        * SSL证书
        * 响应工作流

* requests.head(url, **kwargs)
    * url: URL 
    * **kwargs: Optional arguments that ``request`` takes.
    * return: requests.Response

* requests.post(url, data=None, json=None, **kwargs)
    * url: URL
    * data: (optional) Dictionary (will be form-encoded), bytes, or file-like object to send 
    * json: (optional) json data to send
    **kwargs: Optional arguments that ``request`` takes.
    * return:  requests.Response

    * 字典提交 "form"
    >>> payload = {'key1':'value1', 'key2':['value2','value3']}
    >>> r = requests.post('http://httpbin.org/post', data=payload) #字典提交
    >>> print(r.text)
    {
    ...
    "form": {
        "key1": "value1", 
        "key2": [
        "value2", 
        "value3"
        ]
    }, 
        ...
    }
    *  字符串提交时 'data'
    >>> r = requests.post('http://httpbin.org/post', data=json.dumps(payload)) #字符串提交

    {  
    "args": {}, 
    "data": "{\"key1\": \"value1\", \"key2\": [\"value2\", \"value3\"]}", 
    "form":{},
    ...
    }

    * 传递文件
        >>> url = 'http://httpbin.org/post'
        >>> files = {'file': open('report.xls', 'rb')}
        >>> r = requests.post(url, files=files)
        >>> r.text
        配置files，filename, content_type and headers
        files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}
        files = {'file': ('report.csv', 'some,data,to,send\nanother,row,to,send\n')}
    
    * 流失上传
    * 分块传输

* requests.put(url, data=None, **kwargs)
* requests.patch(url, data=None, **kwargs)
* requests.delete(url, **kwargs)
* requests.option

* requests.Session() 


* Response对象
    *  Response对象的属性：
        * status_code   #状态码 200表示链接成功，404表示失败

        * text          #内容部分(存在编码问题)
        * content       #二进制内容，用来下载图片和视频
        * headers       #头部信息

        * encoding      #从header中猜测的响应内容的编码方式
        * apparent_encoding  #内容分析出的编码

        * url           #返回网址
        * cookies
        * history

    * Response方法：
        * raise_for_status()  #用来抛出异常

* robots协议
    * 网络引发的问题：
        * 性能骚扰： 
            * Web服务器默认接收人类访问受限于编写水平和目的，网络爬虫将会为Web服务器带来巨大的资源开销
        * 法律风险： 
            * 服务器上的数据有产权归属网络爬虫获取数据后牟利将带来法律风险
        * 隐私泄露
            * 网络爬虫可能具备突破简单访问控制的能力，获得被保护数据从而泄露个人隐私
    * 限制：
        * 根据 User-Aget限制
        * robots协议  /robots.txt

  
### BeautifulSoup库
* 安装.
    * 管理员权限打开cmd
    * 执行: pip install beautifulsoup4

* 导入
    from bs4 import BeautifulSoup
    
* 解析
    soup = BeautifulSoup("<html>data</html>","html.parser")

    * bs4的HTML解析器：BeautifulSoup(r.text,'html.parser')——条件：安装bs4库
    * lxml的HTML解析器：BeautifulSoup(r.text,'lxml')            pip install lxml
    * lxml的XML解析器：BeautifulSoup(r.text,'xml')              pip install lxml
    * html5lib的解析器：BeautifulSoup(r.text,'html5lib')        pip install html5lib

* 打印
    * .prettify() 格式化打印
        >>> print(soup.prettify())

* soup的4大对象类型

    * Tag(HTML的标签），最基本的信息组织单元，分别用<>和</>表明开头和结尾
        * 当有多个标签时，只返回第一个标签  

        * 如：
            * soup.title 返回页面标题
            * soup.head
            * soup.a
            * soup.h1
            * soup.p
            * ...html的标签

        * Tag标签的属性: name, attrs
            * .name:  返回标签名字，返回类型为字符串
                >>> soup.head.name 
                >>> head
                >>> soup.name    
                >>> [document]

            * .attrs:  返回标签的属性,返回类型为字典类型 Atrributes
                >>> soup.p.attrs 
                >>> {'class': ['title'], 'name': 'dromouse'}
                >>> soup.p.attrs['class']
                >>> ['title']

    * NavigableString
        * .string 返回标签的内容 
        >>> soup.p.string
        >>> The Dormouse's story
        >>> type(soup.p.string)
        >>> bs4.element.NavigableString

        * 如果tag只有一个NavigableString 类型子节点,那么返回这个子节点的内容
        * 如果tag只有一个子节点,则返回唯一子节点的.string
        * 如果tag包含了多个子节点,.string 的输出结果是 None

         * .strings 获取多个内容，不过需要遍历获取
                >>> for s in soup.strings:
                        print(s)

        * .stripped_strings 可以去除多余空白内容
            >>> for s in soup.stripped_strings:
                    print(s)

    * BeautifulSoup 表示的是一个文档的全部内容,可以把它当做特殊的Tag
        >>> soup.name
        >>> '[document]'
        >>> soup.attrs
        >>> {}

    * Comment 是一个特殊类型的 NavigableString 对象
        >>> soup.a
        >>> <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
        >>> soup.a.string
        >>> ' Elsie '
        >>> type(soup.a.string)
        >>> bs4.element.Comment

* 遍历文档树
    * 上行遍历
        * tag.parent        返回tag的父节点
            >>> soup.a.parent
        * tag.parents		返回tag的所有父辈节点的generator
            >>> for parent in soup.a.parents:
                    print(parent.name)
            >>> p
                body
                html
                [document]

    * 下行遍历
        * tag.contents	返回包含tag的子节点的列表
            >>> soup.head.contents
            >>> [<title>The Dormouse's story</title>, <meta charset='utf-8' />,.....]

        * tag.children	返回tag子节点的list_iterator， 可通过它进行遍历
            >>> for child in soup.body.children:
                    print(child)

        * tag.descendants   返回tag所有子孙节点的generator， 可通过它进行遍历
            >> for child in soup.body.descendants:
                    print(child)

    * 平行遍历
        * tag.next_sibling  返回tag的下个兄弟结点,有可能是空白是换行
            >>> soup.p.next_sibling

        * tag.next_siblings   返回tag的全部兄弟结点的generator，可遍历
            >>> for sibling in soup.p.next_siblings:
                    print(sibling.anme)

        * tag.previous_sibling
        * tag.previous_siblings ->iterator

    * 前后遍历
        * 与 .next_sibling, .previous_sibling 不同，它并不是针对于兄弟节点，而是在所有节点，不分层次
        * .next_element
        * .next_elements
        * .previous_element
        * .previous_elements

* 搜索文档树
    * .find_all(name, attrs, recursive, text, limit, **kwargs) 返回列表，搜索当前结点的所有子节点和孙子结点 
        * name ：可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉
            * 传字符串
            >>> soup.find_all('a')
            * 传正则表达式 re.compile()
            >>> soup.find_all(re.compile('^b'))
                #body
                #b
            * 传列表： [tag1, tag2]
            >>> soup.find_all(["a", "b"])
                #a
                #b
            * True 可以匹配任何值,
            >>> soup.find_all(True)
            * 传方法
            >>> def fun(tag):
                    return tag.has_attr('class') and not tag.has_attr('id')
            >>> soup.find_all(fun)

        * keyword参数, tag的属性如id，href,class_(class是python的关键字)
            >>> soup.find_all(id='link2')
            >>> soup.find_all(class_='sister')
            >>> soup.find_all(href=re.compile("else"))
            >>> soup.find_all('a', href=re.compile("else"), id='link2') #同时过滤

        * attrs  参数定义一个字典参数来搜索包含特殊属性的tag
            >>> soup.find_all(attrs={'id':'link2'})
            >>> soup.find_all(attrs={'class':'sister'})
            >>> soup.find_all(attrs={'href':re.compile("el")})
            >>> soup.find_all(attrs={'id':'link2', 'href':re.compile("el"),}) #同时过滤

        * text 可以搜索文档中的字符串内容,接受 字符串, 正则表达式, 列表, True
            >>> soup.find_all(text='Tillie')
            >>> soup.find_all(text=['Tillie','Lacie'])
            >>> soup.find_all(text=re.compile('ie'))

        * limit 限制返回的数量
            >>> soup.find_all('a', limit=2)

        * recursive True/False,默认是True，是否检索子孙结点
            >>> soup.find_all('a', recursive=False)

    * .find(name, attrs, recursive, text, **kwargs) 直接返回结果，搜索当前结点的所有子节点和孙子结点 ,只返回一个
    * .find_parents(), .find_parnet()   用来搜索当前节点的父辈节点,有's'的返回所有满足的list，没’s'只返回第一个满足的
    * .find_next_siblings(), .find_next_sibling()  用来搜索当前节点下面的兄弟节点,有's'的返回所有满足的list，没’s'只返回第一个满足的
    * .find_previous_siblings() ,.find_previous_sibling()
    * find_all_next(), .find_next()  通过.next_element,  _all__返回所有满足，没all的返回第一个
    * find_all_previous() 和 find_previous() 通过.previous_element,  _all__返回所有满足，没all的返回第一个

* CSS选择器 soup.select 返回 list, 通过get_text()来获得内容 和 find_all不同的查找方式
    * 通过标签名查找
        >>> soup.select('a)
    * 通过类名查找
        >>> soup.select('.sister')
    * 通过 id 名查找
        >>> soup.select('#link1')
    * 组合查找
        >>> soup.select('p #link1')
    * 属性查找
        >>> soup.select('a[class="sister"]')
        >>> soup.select('a[href="http://example.com/elsie"]')
        >>> soup.select('p a[href="http://example.com/elsie"]')

### re（正则表达式)
    import re

re.search(pattern,string,flags=0)
	re.I  	ingorecase
	re.M  	multiline
	re.S	DOTALL

	match = re.search(r'[1-9]\d{5}','BIT 100081')
	if match:
		print(match.group(0))

re.match(pattern,string,flags=0)
	match = re.match(r'[1-9]\d{5}','100061 BIT')
	if match:
		print(match.group(0))

re.findall(pattern,string,flags=0)
	ls = re.findall(r'[1-9]\d{5}','BIT 100061 1000082Buaa')
	ls
	['100061', '100008'] 	

re.split(pattern,string,maxsplit = 0,flags=0)

	re.split(r'[1-9]\d{5}','BIT100081 TSU100084')
	['BIT', ' TSU', '']

	re.split(r'[1-9]\d{5}','BIT100081 TSU100084',1)
	['BIT', ' TSU100084']

re.finditer(pattern,string,flags=0)
	for m in re.finditer('[1-9]\d{5}','BIT100081 TSU100084'):
	if m:
		print(m.group(0))
re.sub(pattern,reol,string,count=0,flags=0)替换
	re.sub(r'[1-9]\d{5}','zipcode','BIT100081 TSU100084')
	'BITzipcode TSUzipcode'

面向对象法：
regex = re.compile(pattern,flags=0)

Match对象
.string
.re
.pos
.endpos

.group(0)
.start()
.end()
.span()


贪婪匹配
最小匹配


### HTTP协议
- HTTP，Hypertext Transfer Protocol，超文本传输协议
- HTTP是一个基于“请求与响应”模式的、无状态的应用层协议  
- URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源
* url格式:http://host[:port][path]
    - host: 合法的Internet主机域名或IP地址
    - port: 端口号，缺省端口为80
    - path: 请求资源的路径
* HTTP协议对资源的操作
    |A|B|
    |:--:|:--:|
    GET |请求获取URL位置的资源
    HEAD |请求获取URL位置资源的响应消息报告，即获得该资源的头部信息
    POST |请求向URL位置的资源后附加新的数据
    PUT |请求向URL位置存储一个资源，覆盖原URL位置的资源
    PATCH |请求局部更新URL位置的资源，即改变该处资源的部分内容
    DELETE |请求删除URL位置存储的资源

* PUT和PATCH的区别
    * 采用PATCH，仅向URL提交UserName的局部更新请求
    * 采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除
    * PATCH的最主要好处：节省网络带宽

### 项目
* 京东商品页面的爬取
* 亚马逊商品页面的爬取
* 百度/360搜索关键字提交
* 网络图片的爬取和存储
* IP地址归属地的自动查询
* 中国大学排名定向爬虫
* 淘宝商品比价定向爬虫
* 股票数据定向爬虫
* 股票数据专业爬虫
* 表情包专业爬虫